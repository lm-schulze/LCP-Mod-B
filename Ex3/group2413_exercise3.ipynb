{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765922f1-b6e3-474a-acfb-b64ca19be901",
   "metadata": {},
   "source": [
    "## Team 2413:\n",
    "- Maryam Gholami Shiri (2013071)\n",
    "- Muhammad Usama Qasim (2040472)\n",
    "- Laura Schulze (2122311)\n",
    "- Savina Tsichli (10738280)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b796e62-e0e6-4e11-aea0-31076ef879a8",
   "metadata": {},
   "source": [
    "# Exercise 3: XGBoost\n",
    "\n",
    "Study the data in the file x_XGB_24.dat (N=2000 samples) with labels y_XGB_24.dat. The dataset should be split into N’ training samples and N’’ validation samples, with N’+N’’=N. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e710b714-76f0-4119-b691-fa4687970573",
   "metadata": {},
   "source": [
    "## 3.1 Model complexity, parameters’ and regularization\n",
    "Try different parameters (λ, γ, n_estimators, …). Which is the simplest yet effective XGBoost model that keeps a good validation accuracy? Is regularization useful for this analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a261a97-6d8a-4222-b7e5-29db9558f629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f30d3e4-d1ca-4393-986a-27128277e338",
   "metadata": {},
   "source": [
    "## 3.2 Dimensionality reduction\n",
    "Consider reduced data samples with L’<L features. For example, feature 0,1, and 3 out of the L=4 features. Check if the exclusion of the least important feature(s) from training data leads to better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420da546-74d4-4107-b944-f4ab3b51c3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71887e2b-91a1-4a84-8611-cf7a6c1ef286",
   "metadata": {},
   "source": [
    "## 3.3 XGBoost vs NN\n",
    "Compare the validation accuracy of XGBoost with that of a simple feed-forward neural network (NN) \n",
    "* By varying the number of data samples N’ in the training set (i.e., reducing the fraction N’/N of the data set used for training) \n",
    "* With cross-validation for all cases.\n",
    "  \n",
    "Is the NN or the XGB performing significantly better at low N’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02605e7-5691-42e2-b4c8-f3899f26c4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "931046b6-9f45-4629-9da8-0436e195ac5c",
   "metadata": {},
   "source": [
    "**Cross-validation** collects the statistics from multiple realizations of training and validation, each performed for a different\n",
    "selection of the training set. For example, one can leave out a given block [0,1,2,…, N’’-1] of data samples for validation\n",
    "and train on the remaining samples. The procedure is iterated for the next block [N’’,…, 2N’’-1], etc., so that, in total, N/N’’\n",
    "independent training and validations are performed with the same full dataset. Another possibility is randomly picking the\n",
    "validation samples. As a result of cross-validation, one gets an error estimate for the accuracy of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
