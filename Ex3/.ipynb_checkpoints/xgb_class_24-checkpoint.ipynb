{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# AdaBoost Algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Gradient Boosting \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# XGBoost \n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance, to_graphviz, plot_tree\n",
    "print(\"XGBoost version:\",xgboost.__version__)\n",
    "\n",
    "mycmap = \"winter\"\n",
    "mpl.rcParams['image.cmap'] = mycmap\n",
    "plt.rcParams['font.size'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "dname=\"./DATA/\"\n",
    "str0=\"_XGB_24.dat\"\n",
    "fnamex=dname+'x'+str0\n",
    "fnamey=dname+'y'+str0\n",
    "x = np.loadtxt(fnamex, delimiter=\" \",dtype=float)\n",
    "y = np.loadtxt(fnamey)\n",
    "y = y.astype(int)\n",
    "N,L = len(x), len(x[0])\n",
    "\n",
    "N_train = int(0.75*N)\n",
    "x_train,y_train = x[:N_train],y[:N_train]\n",
    "x_test,y_test = x[N_train:],y[N_train:]\n",
    "print(f\"N={N}, N_train={N_train}, L={L}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scat(ax,x,y,i=0,j=1,s=4,title=\"\"):\n",
    "    ax.scatter(x[:,i],x[:,j],s=s,c=y)\n",
    "    ax.set_xlabel(f\"feature {i}\")\n",
    "    ax.set_ylabel(f\"feature {j}\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "for n in range(5):\n",
    "    print(x[n],y[n])\n",
    "\n",
    "fig,AX = plt.subplots(2,2,figsize=(8.5,8.1))\n",
    "scat(AX[0,0],x_train,y_train,title=\"Train\")\n",
    "scat(AX[0,1],x_train,y_train,i=2,j=3,title=\"Train\")\n",
    "scat(AX[1,0],x_test,y_test,title=\"Test\")\n",
    "scat(AX[1,1],x_test,y_test,i=2,j=3,title=\"Test\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(clf=GradientBoostingClassifier(),show=False):\n",
    "    # GradientBoostingClassifier():\n",
    "    #   n_estimators = 100 (default)\n",
    "    #   loss function = deviance(default) used in Logistic Regression\n",
    "    # XGBClassifier()\n",
    "    #   n_estimators = 100 (default)\n",
    "    #   max_depth = 3 (default?)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_hat = clf.predict(x_test)\n",
    "    \n",
    "    print(\"errors: {:.2f}%   Accuracy={:.3f}\".format(100*(1-clf.score(x_test, y_test)),clf.score(x_test, y_test)))\n",
    "    S=50\n",
    "    dx = 1\n",
    "    x_seq=np.arange(-S,S+dx,dx)\n",
    "    nx = len(x_seq)\n",
    "    x_plot=np.zeros((nx*nx,L))\n",
    "    q=0\n",
    "    for i in range(nx):\n",
    "        for j in range(nx):\n",
    "            x_plot[q,:2] = [x_seq[i],x_seq[j]]\n",
    "            q+=1\n",
    "    y_plot= clf.predict(x_plot)\n",
    "\n",
    "    fig,AX = plt.subplots(1,2,figsize=(8.2,4))\n",
    "    scat(AX[0],x_plot[:],y_plot,s=dx,title=\"predicted\")\n",
    "    scat(AX[1],x_train[:],y_train,title=\"training set\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if show:      \n",
    "        dump_list = clf.get_booster().get_dump()\n",
    "        num_trees = len(dump_list)\n",
    "        print(\"num_trees=\",num_trees)\n",
    "        \n",
    "        fig, AX = plt.subplots(2,1,figsize=(12, 5))\n",
    "        for i in range(min(2,num_trees)):\n",
    "            ax=AX[i]\n",
    "            plot_tree(clf, num_trees=i, ax=ax)\n",
    "        fig.savefig(\"DATA/tree-classif.png\", dpi=400, pad_inches=0.02)   \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify\n",
    "If the “objective” hyperparameter is left unspecified, XGBClassifier looks at the data and chooses automatically the loss functions and the evaluation metrics (--> WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "https://xgboost.readthedocs.io/en/stable/python/python_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(seed=1,\n",
    "                      objective='binary:logistic') \n",
    "       # importance_type=\"gain\" #weight, cover, ...\n",
    "       # learning_rate=0.4,\n",
    "       # reg_lambda=0.001, \n",
    "       # n_estimators=30)\n",
    "\n",
    "classify(model, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/\n",
    "# feature importance\n",
    "\n",
    "print(model.importance_type)\n",
    "print(model.feature_importances_)\n",
    "# plot\n",
    "my_cmap = plt.get_cmap(\"Reds\")\n",
    "rescale = lambda y: 0.3 + 0.7 * (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_,\n",
    "        color=my_cmap(rescale(model.feature_importances_)))\n",
    "plt.xlabel(\"feature\")\n",
    "plt.ylabel(\"importance\")\n",
    "plt.xticks(np.arange(L))\n",
    "plt.title(model.importance_type)\n",
    "plt.show()\n",
    "\n",
    "rescale_r = lambda y: 0.3 + 0.7 * (np.max(y) - y) / (np.max(y) - np.min(y))\n",
    "plot_importance(model,color=my_cmap(rescale_r(model.feature_importances_)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation: https://xgboost.readthedocs.io/en/stable/python/python_api.html\n",
    "\n",
    "    ‘weight’: the number of times a feature is used to split the data across all trees.\n",
    "\n",
    "    ‘gain’: the average gain across all splits the feature is used in.\n",
    "\n",
    "    ‘cover’: the average coverage across all splits the feature is used in.\n",
    "\n",
    "    ‘total_gain’: the total gain across all splits the feature is used in.\n",
    "\n",
    "    ‘total_cover’: the total coverage across all splits the feature is used in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the importance type is “total_gain”, then the score is sum of loss change for each split from all trees.\n",
    "list_type=[\"weight\",\"gain\",\"total_gain\",\"cover\",\"total_cover\"]\n",
    "list_col=[\"#7565F0\",\"k\",\"#666666\",\"#F0A000\",\"gold\"]\n",
    "\n",
    "for i,t in enumerate(list_type):\n",
    "    feature_imp = model.get_booster().get_score(importance_type=t)\n",
    "    keys = list(feature_imp.keys())\n",
    "    values = np.array(list(feature_imp.values()))\n",
    "    print(i,t,values)\n",
    "    values= values/np.sum(values)\n",
    "    plt.bar(np.arange(L)+(i-L/2)/10, values,color=list_col[i],width=0.1,label=t)\n",
    "plt.xlabel(\"feature\")\n",
    "plt.ylabel(\"importance (normalized to 1)\")\n",
    "plt.xticks(np.arange(L))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier()\n",
    "classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(clf=AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
